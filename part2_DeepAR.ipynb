{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Workflow\n",
    "1. Loading the data\n",
    "2. Creating training and test sets of time series\n",
    "3. Formatting data as JSON files and uploading to S3\n",
    "4. Instantiating and training a DeepAR estimator\n",
    "5. Deploying a model and creating a predictor\n",
    "6. Evaluating the predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Store</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Date</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Open</th>\n",
       "      <th>Promo</th>\n",
       "      <th>StateHoliday</th>\n",
       "      <th>SchoolHoliday</th>\n",
       "      <th>ID</th>\n",
       "      <th>StoreType</th>\n",
       "      <th>Assortment</th>\n",
       "      <th>CompetitionDistance</th>\n",
       "      <th>Promo_2_active</th>\n",
       "      <th>Open_sunday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>5263</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1270</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>6064</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>570</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>8314</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14130</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>13995</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>620</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>4822</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29910</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Store  DayOfWeek        Date  Sales  Open  Promo  StateHoliday  \\\n",
       "0           0      1          5  2015-07-31   5263     1      1             1   \n",
       "1           1      2          5  2015-07-31   6064     1      1             1   \n",
       "2           2      3          5  2015-07-31   8314     1      1             1   \n",
       "3           3      4          5  2015-07-31  13995     1      1             1   \n",
       "4           4      5          5  2015-07-31   4822     1      1             1   \n",
       "\n",
       "   SchoolHoliday  ID  StoreType  Assortment  CompetitionDistance  \\\n",
       "0              1   0          2           0                 1270   \n",
       "1              1   1          0           0                  570   \n",
       "2              1   2          0           0                14130   \n",
       "3              1   3          2           2                  620   \n",
       "4              1   4          0           0                29910   \n",
       "\n",
       "   Promo_2_active  Open_sunday  \n",
       "0               0            0  \n",
       "1               1            0  \n",
       "2               1            0  \n",
       "3               0            0  \n",
       "4               0            0  "
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data \n",
    "df = pd.read_csv('prepared_data_all.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change order of df so latest date comes first\n",
    "df = df.iloc[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Date</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Open</th>\n",
       "      <th>Promo</th>\n",
       "      <th>StateHoliday</th>\n",
       "      <th>SchoolHoliday</th>\n",
       "      <th>ID</th>\n",
       "      <th>StoreType</th>\n",
       "      <th>Assortment</th>\n",
       "      <th>CompetitionDistance</th>\n",
       "      <th>Promo_2_active</th>\n",
       "      <th>Open_sunday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1017208</th>\n",
       "      <td>1114</td>\n",
       "      <td>2</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1017208</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5350</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017207</th>\n",
       "      <td>1113</td>\n",
       "      <td>2</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1017207</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>870</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017206</th>\n",
       "      <td>1112</td>\n",
       "      <td>2</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1017206</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9260</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017205</th>\n",
       "      <td>1111</td>\n",
       "      <td>2</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1017205</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1880</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017204</th>\n",
       "      <td>1110</td>\n",
       "      <td>2</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1017204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1900</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Store  DayOfWeek        Date  Sales  Open  Promo  StateHoliday  \\\n",
       "1017208   1114          2  2013-01-01      0     0      0             1   \n",
       "1017207   1113          2  2013-01-01      0     0      0             1   \n",
       "1017206   1112          2  2013-01-01      0     0      0             1   \n",
       "1017205   1111          2  2013-01-01      0     0      0             1   \n",
       "1017204   1110          2  2013-01-01      0     0      0             1   \n",
       "\n",
       "         SchoolHoliday       ID  StoreType  Assortment  CompetitionDistance  \\\n",
       "1017208              1  1017208          3           2                 5350   \n",
       "1017207              1  1017207          0           2                  870   \n",
       "1017206              1  1017206          0           2                 9260   \n",
       "1017205              1  1017205          2           2                 1880   \n",
       "1017204              1  1017204          0           0                 1900   \n",
       "\n",
       "         Promo_2_active  Open_sunday  \n",
       "1017208               0            0  \n",
       "1017207               0            0  \n",
       "1017206               0            0  \n",
       "1017205               0            0  \n",
       "1017204               0            0  "
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for the DeepAr Algorithm, Categorical features must be encoded as a 0-based sequence of positive integers\n",
    "# thus the stores (which is the category I want to distinguish in this time series) needs to start from zero and not from 1\n",
    "\n",
    "# reduce all store numbers by one\n",
    "df ['Store'] =  df.Store.apply(lambda x: (x-1))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter into train/test before running transformation\n",
    "\n",
    "# as defined in Proposal: \n",
    "# train range 07.01.2013 – 07.06.2015 (94,7% of data)\n",
    "# test range 08.06.2015-26.07.2015 (5,3% of data) \n",
    "\n",
    "# TRAIN\n",
    "# Filter out all rows with a date past 07.06.2015\n",
    "df_train = df[df['Date']<'2015-06-08']\n",
    "# Filter out all rows with a date before 07.01.2013\n",
    "df_train = df_train[df_train['Date']>='2013-07-01']\n",
    "\n",
    "# TEST\n",
    "# !!!!! for this algorithm, the test set contains the complete range of each time series.!!!!!\n",
    "# Filter out all rows with a date before 08.06.2015\n",
    "df_test = df[df['Date']>='2013-07-01']\n",
    "# Filter out all rows with a date past 26.07.2015\n",
    "df_test = df_test[df_test['Date']<'2015-07-27']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Date'] = pd.to_datetime(df_train.Date)\n",
    "df_test['Date'] = pd.to_datetime(df_test.Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.set_index('Date')\n",
    "df_test = df_test.set_index('Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to JASON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset a list of Stores to iterate over\n",
    "store_nr = list(df_train['Store'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json for formatting data and os for saving\n",
    "import json\n",
    "import os \n",
    "\n",
    "# transforming df\n",
    "\n",
    "def write_json_dataset(df, filename): \n",
    "    with open(filename, 'wb') as f:\n",
    "        # for each of our times series, there is one JSON line\n",
    "        for store in store_nr:\n",
    "            df_store = df.loc[df['Store'] == store]\n",
    "            obj = {\"start\": str(df_store.index[0]), \"target\": list(df_store.Sales), \"cat\": [int(store)], \"dynamic_feat\": [list(df_store.DayOfWeek),list(df_store.Open),list(df_store.Promo),list(df_store.StateHoliday),list(df_store.SchoolHoliday),list(df_store.StoreType),list(df_store.Assortment),list(df_store.CompetitionDistance),list(df_store.Promo_2_active),list(df_store.Open_sunday)]}\n",
    "            json_line = json.dumps(obj) + '\\n'\n",
    "            json_line = json_line.encode('utf-8')\n",
    "            f.write(json_line)\n",
    "    print(filename + ' saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this data to a local directory\n",
    "data_dir = 'json_rossmann'\n",
    "\n",
    "# make data dir, if it does not exist\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json_rossmann/train.json saved.\n",
      "json_rossmann/test.json saved.\n"
     ]
    }
   ],
   "source": [
    "# directories to save train/test data\n",
    "train_key = os.path.join(data_dir, 'train.json')\n",
    "test_key = os.path.join(data_dir, 'test.json')\n",
    "\n",
    "# write train/test JSON files\n",
    "write_json_dataset(df_train, train_key)        \n",
    "write_json_dataset(df_test, test_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session, role, bucket\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general prefix\n",
    "prefix='deepar-rossmann'\n",
    "\n",
    "# *unique* train/test prefixes\n",
    "train_prefix   = '{}/{}'.format(prefix, 'train')\n",
    "test_prefix    = '{}/{}'.format(prefix, 'test')\n",
    "\n",
    "# uploading data to S3, and saving locations\n",
    "train_path  = sagemaker_session.upload_data(train_key, bucket=bucket, key_prefix=train_prefix)\n",
    "test_path   = sagemaker_session.upload_data(test_key,  bucket=bucket, key_prefix=test_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data is stored in: s3://sagemaker-eu-central-1-395339144106/deepar-rossmann/train/train.json\n",
      "Test data is stored in: s3://sagemaker-eu-central-1-395339144106/deepar-rossmann/test/test.json\n"
     ]
    }
   ],
   "source": [
    "# check locations\n",
    "print('Training data is stored in: '+ train_path)\n",
    "print('Test data is stored in: '+ test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking a preview of the json the data at s3 through the aws console shows, that all the data is in the right format as indicated in the aws documentation: https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling DeepAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "image_name = get_image_uri(boto3.Session().region_name, # get the region\n",
    "                           'forecasting-deepar') # specify image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# dir to save model artifacts\n",
    "s3_output_path = \"s3://{}/{}/output\".format(bucket, prefix)\n",
    "\n",
    "# instantiate a DeepAR estimator\n",
    "estimator = Estimator(sagemaker_session=sagemaker_session,\n",
    "                      image_name=image_name,\n",
    "                      role=role,\n",
    "                      train_instance_count=1,\n",
    "                      train_instance_type='ml.p2.xlarge',\n",
    "                      output_path=s3_output_path\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq='D'\n",
    "prediction_length= 49 # number of days in test data set (7 weeks x 7 days)\n",
    "context_length= 490 # less then number of days in train data set (126 weeks x 7 days); \n",
    "# \"a model can look further back in the time series than the value specified for context_length\"\n",
    "epochs = 50 # the maximum number of times to pass over the data when training\n",
    "# Further parameter explenation: https://docs.aws.amazon.com/forecast/latest/dg/aws-forecast-recipe-deeparplus.html \n",
    "\n",
    "hyperparameters = {\n",
    "    \"epochs\": str(epochs),\n",
    "    \"time_freq\": freq,\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "    \"context_length\": str(context_length),\n",
    "    \"num_cells\": \"50\",\n",
    "    \"num_layers\": \"3\",\n",
    "    \"mini_batch_size\": \"128\",\n",
    "    \"learning_rate\": \"0.001\",\n",
    "    \"early_stopping_patience\": \"10\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the hyperparams\n",
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-07 14:27:32 Starting - Starting the training job...\n",
      "2020-04-07 14:27:34 Starting - Launching requested ML instances...\n",
      "2020-04-07 14:28:29 Starting - Preparing the instances for training.........\n",
      "2020-04-07 14:29:50 Downloading - Downloading input data...\n",
      "2020-04-07 14:30:05 Training - Downloading the training image....\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:07 INFO 140569346963264] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:07 INFO 140569346963264] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'0.001', u'num_cells': u'50', u'prediction_length': u'49', u'epochs': u'50', u'time_freq': u'D', u'context_length': u'490', u'num_layers': u'3', u'mini_batch_size': u'128', u'early_stopping_patience': u'10'}\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:07 INFO 140569346963264] Final configuration: {u'dropout_rate': u'0.10', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_layers': u'3', u'epochs': u'50', u'embedding_dimension': u'10', u'num_cells': u'50', u'_num_kv_servers': u'auto', u'mini_batch_size': u'128', u'likelihood': u'student-t', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'49', u'time_freq': u'D', u'context_length': u'490', u'_kvstore': u'auto', u'early_stopping_patience': u'10'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:07 INFO 140569346963264] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:07 INFO 140569346963264] Using early stopping with patience 10\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:07 INFO 140569346963264] [cardinality=auto] `cat` field was found in the file `/opt/ml/input/data/train/train.json` and will be used for training.\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:07 INFO 140569346963264] [num_dynamic_feat=auto] `dynamic_feat` field was found in the file `/opt/ml/input/data/train/train.json` and will be used for training.\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:08 INFO 140569346963264] [cardinality=auto] Inferred value of cardinality=[1115] from dataset.\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:08 INFO 140569346963264] [num_dynamic_feat=auto] Inferred value of num_dynamic_feat=10 from dataset.\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:08 INFO 140569346963264] Training set statistics:\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:08 INFO 140569346963264] Integer time series\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:08 INFO 140569346963264] number of time series: 1115\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:08 INFO 140569346963264] number of observations: 755185\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:08 INFO 140569346963264] mean target length: 677\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:08 INFO 140569346963264] min/mean/max target: 0.0/5829.70222528/38722.0\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:08 INFO 140569346963264] mean abs(target): 5829.70222528\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:08 INFO 140569346963264] contains missing values: no\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:08 INFO 140569346963264] Small number of time series. Doing 2 passes over dataset with prob 0.57399103139 per epoch.\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:09 INFO 140569346963264] Test set statistics:\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:09 INFO 140569346963264] Integer time series\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:09 INFO 140569346963264] number of time series: 1115\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:09 INFO 140569346963264] number of observations: 809820\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:09 INFO 140569346963264] mean target length: 726\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:09 INFO 140569346963264] min/mean/max target: 0.0/5832.84927885/41551.0\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:09 INFO 140569346963264] mean abs(target): 5832.84927885\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:09 INFO 140569346963264] contains missing values: no\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:09 INFO 140569346963264] nvidia-smi took: 0.0503849983215 secs to identify 1 gpus\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:09 INFO 140569346963264] Number of GPUs being used: 1\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:09 INFO 140569346963264] Create Store: local\u001b[0m\n",
      "\n",
      "2020-04-07 14:31:05 Training - Training image download completed. Training in progress.\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 10501.61099433899, \"sum\": 10501.61099433899, \"min\": 10501.61099433899}}, \"EndTime\": 1586269880.159562, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586269869.656855}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:20 INFO 140569346963264] Number of GPUs being used: 1\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 18317.680835723877, \"sum\": 18317.680835723877, \"min\": 18317.680835723877}}, \"EndTime\": 1586269887.974706, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586269880.159653}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:33 INFO 140569346963264] Epoch[0] Batch[0] avg_epoch_loss=6.837313\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:33 INFO 140569346963264] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=6.83731269836\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:45 INFO 140569346963264] Epoch[0] Batch[5] avg_epoch_loss=6.397118\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:45 INFO 140569346963264] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=6.39711825053\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:45 INFO 140569346963264] Epoch[0] Batch [5]#011Speed: 50.40 samples/sec#011loss=6.397118\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:57 INFO 140569346963264] Epoch[0] Batch[10] avg_epoch_loss=6.619004\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:57 INFO 140569346963264] #quality_metric: host=algo-1, epoch=0, batch=10 train loss <loss>=6.88526697159\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:57 INFO 140569346963264] Epoch[0] Batch [10]#011Speed: 53.90 samples/sec#011loss=6.885267\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:57 INFO 140569346963264] processed a total of 1306 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 50, \"sum\": 50.0, \"min\": 50}, \"update.time\": {\"count\": 1, \"max\": 29812.965154647827, \"sum\": 29812.965154647827, \"min\": 29812.965154647827}}, \"EndTime\": 1586269917.787848, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586269887.974786}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:57 INFO 140569346963264] #throughput_metric: host=algo-1, train throughput=43.806240496 records/second\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:57 INFO 140569346963264] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:57 INFO 140569346963264] #quality_metric: host=algo-1, epoch=0, train loss <loss>=6.61900403283\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:57 INFO 140569346963264] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:31:58 INFO 140569346963264] Saved checkpoint to \"/opt/ml/model/state_cb97a09d-a4b5-4955-b02f-9e8f7e85f9b9-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 274.44005012512207, \"sum\": 274.44005012512207, \"min\": 274.44005012512207}}, \"EndTime\": 1586269918.06306, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586269917.787946}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:32:01 INFO 140569346963264] Epoch[1] Batch[0] avg_epoch_loss=6.406162\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:32:01 INFO 140569346963264] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=6.40616178513\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:32:13 INFO 140569346963264] Epoch[1] Batch[5] avg_epoch_loss=6.324915\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:32:13 INFO 140569346963264] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=6.32491532962\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:32:13 INFO 140569346963264] Epoch[1] Batch [5]#011Speed: 53.85 samples/sec#011loss=6.324915\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:32:23 INFO 140569346963264] processed a total of 1261 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 25631.50691986084, \"sum\": 25631.50691986084, \"min\": 25631.50691986084}}, \"EndTime\": 1586269943.694712, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586269918.063141}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:32:23 INFO 140569346963264] #throughput_metric: host=algo-1, train throughput=49.1970346964 records/second\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:32:23 INFO 140569346963264] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:32:23 INFO 140569346963264] #quality_metric: host=algo-1, epoch=1, train loss <loss>=6.16975970268\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:32:23 INFO 140569346963264] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:32:23 INFO 140569346963264] Saved checkpoint to \"/opt/ml/model/state_afeb8b21-e4d1-401b-85a7-3158acb9911c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 285.0019931793213, \"sum\": 285.0019931793213, \"min\": 285.0019931793213}}, \"EndTime\": 1586269943.980455, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586269943.694796}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:32:27 INFO 140569346963264] Epoch[2] Batch[0] avg_epoch_loss=5.556056\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:32:27 INFO 140569346963264] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=5.55605649948\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/07/2020 14:32:40 INFO 140569346963264] Epoch[2] Batch[5] avg_epoch_loss=6.056408\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:32:40 INFO 140569346963264] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=6.05640816689\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:32:40 INFO 140569346963264] Epoch[2] Batch [5]#011Speed: 50.39 samples/sec#011loss=6.056408\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:32:52 INFO 140569346963264] Epoch[2] Batch[10] avg_epoch_loss=5.991790\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:32:52 INFO 140569346963264] #quality_metric: host=algo-1, epoch=2, batch=10 train loss <loss>=5.91424732208\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:32:52 INFO 140569346963264] Epoch[2] Batch [10]#011Speed: 53.76 samples/sec#011loss=5.914247\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:32:52 INFO 140569346963264] processed a total of 1339 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28067.676067352295, \"sum\": 28067.676067352295, \"min\": 28067.676067352295}}, \"EndTime\": 1586269972.048274, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586269943.980533}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:32:52 INFO 140569346963264] #throughput_metric: host=algo-1, train throughput=47.7059189253 records/second\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:32:52 INFO 140569346963264] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:32:52 INFO 140569346963264] #quality_metric: host=algo-1, epoch=2, train loss <loss>=5.99178960107\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:32:52 INFO 140569346963264] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:32:52 INFO 140569346963264] Saved checkpoint to \"/opt/ml/model/state_972836f4-9fc1-45c2-8930-4426e17b36f6-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 285.98499298095703, \"sum\": 285.98499298095703, \"min\": 285.98499298095703}}, \"EndTime\": 1586269972.334861, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586269972.048355}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:32:55 INFO 140569346963264] Epoch[3] Batch[0] avg_epoch_loss=6.036475\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:32:55 INFO 140569346963264] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=6.03647518158\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:33:08 INFO 140569346963264] Epoch[3] Batch[5] avg_epoch_loss=6.045135\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:33:08 INFO 140569346963264] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=6.04513510068\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:33:08 INFO 140569346963264] Epoch[3] Batch [5]#011Speed: 50.39 samples/sec#011loss=6.045135\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:33:18 INFO 140569346963264] processed a total of 1238 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 25676.923990249634, \"sum\": 25676.923990249634, \"min\": 25676.923990249634}}, \"EndTime\": 1586269998.011943, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586269972.334948}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:33:18 INFO 140569346963264] #throughput_metric: host=algo-1, train throughput=48.2142835442 records/second\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:33:18 INFO 140569346963264] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:33:18 INFO 140569346963264] #quality_metric: host=algo-1, epoch=3, train loss <loss>=6.01099419594\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:33:18 INFO 140569346963264] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:33:21 INFO 140569346963264] Epoch[4] Batch[0] avg_epoch_loss=5.717105\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:33:21 INFO 140569346963264] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=5.7171049118\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:33:34 INFO 140569346963264] Epoch[4] Batch[5] avg_epoch_loss=5.986390\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:33:34 INFO 140569346963264] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=5.98638995488\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:33:34 INFO 140569346963264] Epoch[4] Batch [5]#011Speed: 50.39 samples/sec#011loss=5.986390\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:33:46 INFO 140569346963264] Epoch[4] Batch[10] avg_epoch_loss=5.953538\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:33:46 INFO 140569346963264] #quality_metric: host=algo-1, epoch=4, batch=10 train loss <loss>=5.91411533356\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:33:46 INFO 140569346963264] Epoch[4] Batch [10]#011Speed: 53.70 samples/sec#011loss=5.914115\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:33:46 INFO 140569346963264] processed a total of 1301 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28068.567037582397, \"sum\": 28068.567037582397, \"min\": 28068.567037582397}}, \"EndTime\": 1586270026.081152, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586269998.012013}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:33:46 INFO 140569346963264] #throughput_metric: host=algo-1, train throughput=46.3505662955 records/second\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:33:46 INFO 140569346963264] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:33:46 INFO 140569346963264] #quality_metric: host=algo-1, epoch=4, train loss <loss>=5.95353785428\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:33:46 INFO 140569346963264] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:33:46 INFO 140569346963264] Saved checkpoint to \"/opt/ml/model/state_3f96ff9f-a4a2-45bb-ac4f-cfef23200b3c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 273.270845413208, \"sum\": 273.270845413208, \"min\": 273.270845413208}}, \"EndTime\": 1586270026.355129, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586270026.081237}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:33:49 INFO 140569346963264] Epoch[5] Batch[0] avg_epoch_loss=5.961465\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:33:49 INFO 140569346963264] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=5.9614648819\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:34:01 INFO 140569346963264] Epoch[5] Batch[5] avg_epoch_loss=5.992235\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:34:01 INFO 140569346963264] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=5.99223518372\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:34:01 INFO 140569346963264] Epoch[5] Batch [5]#011Speed: 53.90 samples/sec#011loss=5.992235\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:34:14 INFO 140569346963264] Epoch[5] Batch[10] avg_epoch_loss=5.942441\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:34:14 INFO 140569346963264] #quality_metric: host=algo-1, epoch=5, batch=10 train loss <loss>=5.88268890381\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:34:14 INFO 140569346963264] Epoch[5] Batch [10]#011Speed: 50.24 samples/sec#011loss=5.882689\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:34:14 INFO 140569346963264] processed a total of 1360 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28094.93613243103, \"sum\": 28094.93613243103, \"min\": 28094.93613243103}}, \"EndTime\": 1586270054.450208, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586270026.355209}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:34:14 INFO 140569346963264] #throughput_metric: host=algo-1, train throughput=48.4070760533 records/second\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:34:14 INFO 140569346963264] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:34:14 INFO 140569346963264] #quality_metric: host=algo-1, epoch=5, train loss <loss>=5.94244142012\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:34:14 INFO 140569346963264] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:34:14 INFO 140569346963264] Saved checkpoint to \"/opt/ml/model/state_0138f636-2f26-49dc-8e59-17c7faf9797d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 280.22217750549316, \"sum\": 280.22217750549316, \"min\": 280.22217750549316}}, \"EndTime\": 1586270054.731113, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586270054.450298}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:34:18 INFO 140569346963264] Epoch[6] Batch[0] avg_epoch_loss=6.165826\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:34:18 INFO 140569346963264] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=6.16582632065\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:34:30 INFO 140569346963264] Epoch[6] Batch[5] avg_epoch_loss=6.040994\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:34:30 INFO 140569346963264] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=6.04099432627\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:34:30 INFO 140569346963264] Epoch[6] Batch [5]#011Speed: 50.46 samples/sec#011loss=6.040994\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:34:40 INFO 140569346963264] processed a total of 1250 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 25654.242992401123, \"sum\": 25654.242992401123, \"min\": 25654.242992401123}}, \"EndTime\": 1586270080.385475, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586270054.731171}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:34:40 INFO 140569346963264] #throughput_metric: host=algo-1, train throughput=48.7246307819 records/second\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:34:40 INFO 140569346963264] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:34:40 INFO 140569346963264] #quality_metric: host=algo-1, epoch=6, train loss <loss>=6.06124644279\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:34:40 INFO 140569346963264] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:34:43 INFO 140569346963264] Epoch[7] Batch[0] avg_epoch_loss=5.877190\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:34:43 INFO 140569346963264] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=5.87718963623\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/07/2020 14:34:56 INFO 140569346963264] Epoch[7] Batch[5] avg_epoch_loss=6.059514\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:34:56 INFO 140569346963264] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=6.05951364835\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:34:56 INFO 140569346963264] Epoch[7] Batch [5]#011Speed: 50.52 samples/sec#011loss=6.059514\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:35:06 INFO 140569346963264] processed a total of 1264 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 25645.713090896606, \"sum\": 25645.713090896606, \"min\": 25645.713090896606}}, \"EndTime\": 1586270106.031816, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586270080.385567}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:35:06 INFO 140569346963264] #throughput_metric: host=algo-1, train throughput=49.2867882102 records/second\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:35:06 INFO 140569346963264] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:35:06 INFO 140569346963264] #quality_metric: host=algo-1, epoch=7, train loss <loss>=6.02592682838\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:35:06 INFO 140569346963264] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:35:09 INFO 140569346963264] Epoch[8] Batch[0] avg_epoch_loss=6.467091\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:35:09 INFO 140569346963264] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=6.46709108353\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:35:22 INFO 140569346963264] Epoch[8] Batch[5] avg_epoch_loss=5.988136\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:35:22 INFO 140569346963264] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=5.98813565572\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:35:22 INFO 140569346963264] Epoch[8] Batch [5]#011Speed: 50.49 samples/sec#011loss=5.988136\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:35:31 INFO 140569346963264] processed a total of 1255 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 25621.01697921753, \"sum\": 25621.01697921753, \"min\": 25621.01697921753}}, \"EndTime\": 1586270131.653518, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586270106.031887}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:35:31 INFO 140569346963264] #throughput_metric: host=algo-1, train throughput=48.9829631909 records/second\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:35:31 INFO 140569346963264] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:35:31 INFO 140569346963264] #quality_metric: host=algo-1, epoch=8, train loss <loss>=5.92077093124\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:35:31 INFO 140569346963264] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:35:31 INFO 140569346963264] Saved checkpoint to \"/opt/ml/model/state_4ca28f71-3e9d-4e5c-a91e-aeec5baca44c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 282.7620506286621, \"sum\": 282.7620506286621, \"min\": 282.7620506286621}}, \"EndTime\": 1586270131.937009, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586270131.653612}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:35:35 INFO 140569346963264] Epoch[9] Batch[0] avg_epoch_loss=5.806051\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:35:35 INFO 140569346963264] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=5.80605125427\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:35:47 INFO 140569346963264] Epoch[9] Batch[5] avg_epoch_loss=5.981757\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:35:47 INFO 140569346963264] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=5.98175684611\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:35:47 INFO 140569346963264] Epoch[9] Batch [5]#011Speed: 53.92 samples/sec#011loss=5.981757\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:35:59 INFO 140569346963264] Epoch[9] Batch[10] avg_epoch_loss=6.212677\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:35:59 INFO 140569346963264] #quality_metric: host=algo-1, epoch=9, batch=10 train loss <loss>=6.48978023529\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:35:59 INFO 140569346963264] Epoch[9] Batch [10]#011Speed: 50.56 samples/sec#011loss=6.489780\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:35:59 INFO 140569346963264] processed a total of 1322 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27997.10702896118, \"sum\": 27997.10702896118, \"min\": 27997.10702896118}}, \"EndTime\": 1586270159.934268, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586270131.937089}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:35:59 INFO 140569346963264] #throughput_metric: host=algo-1, train throughput=47.2188829414 records/second\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:35:59 INFO 140569346963264] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:35:59 INFO 140569346963264] #quality_metric: host=algo-1, epoch=9, train loss <loss>=6.21267656846\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:35:59 INFO 140569346963264] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:36:03 INFO 140569346963264] Epoch[10] Batch[0] avg_epoch_loss=6.106126\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:36:03 INFO 140569346963264] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=6.10612630844\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:36:16 INFO 140569346963264] Epoch[10] Batch[5] avg_epoch_loss=6.089674\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:36:16 INFO 140569346963264] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=6.08967399597\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:36:16 INFO 140569346963264] Epoch[10] Batch [5]#011Speed: 50.39 samples/sec#011loss=6.089674\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:36:25 INFO 140569346963264] processed a total of 1274 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 25681.590795516968, \"sum\": 25681.590795516968, \"min\": 25681.590795516968}}, \"EndTime\": 1586270185.616501, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586270159.934394}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:36:25 INFO 140569346963264] #throughput_metric: host=algo-1, train throughput=49.6072536014 records/second\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:36:25 INFO 140569346963264] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:36:25 INFO 140569346963264] #quality_metric: host=algo-1, epoch=10, train loss <loss>=6.08169984818\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:36:25 INFO 140569346963264] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:36:29 INFO 140569346963264] Epoch[11] Batch[0] avg_epoch_loss=5.854833\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:36:29 INFO 140569346963264] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=5.85483264923\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:36:41 INFO 140569346963264] Epoch[11] Batch[5] avg_epoch_loss=5.895080\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:36:41 INFO 140569346963264] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=5.89507961273\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:36:41 INFO 140569346963264] Epoch[11] Batch [5]#011Speed: 50.45 samples/sec#011loss=5.895080\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:36:53 INFO 140569346963264] Epoch[11] Batch[10] avg_epoch_loss=5.958801\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:36:53 INFO 140569346963264] #quality_metric: host=algo-1, epoch=11, batch=10 train loss <loss>=6.03526592255\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:36:53 INFO 140569346963264] Epoch[11] Batch [10]#011Speed: 53.73 samples/sec#011loss=6.035266\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:36:53 INFO 140569346963264] processed a total of 1282 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28031.55517578125, \"sum\": 28031.55517578125, \"min\": 28031.55517578125}}, \"EndTime\": 1586270213.648709, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586270185.616596}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:36:53 INFO 140569346963264] #throughput_metric: host=algo-1, train throughput=45.7339744479 records/second\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:36:53 INFO 140569346963264] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:36:53 INFO 140569346963264] #quality_metric: host=algo-1, epoch=11, train loss <loss>=5.95880066265\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:36:53 INFO 140569346963264] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:36:57 INFO 140569346963264] Epoch[12] Batch[0] avg_epoch_loss=6.072583\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:36:57 INFO 140569346963264] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=6.07258319855\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:37:09 INFO 140569346963264] Epoch[12] Batch[5] avg_epoch_loss=5.994571\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:37:09 INFO 140569346963264] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=5.99457073212\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:37:09 INFO 140569346963264] Epoch[12] Batch [5]#011Speed: 50.46 samples/sec#011loss=5.994571\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/07/2020 14:37:21 INFO 140569346963264] Epoch[12] Batch[10] avg_epoch_loss=6.061360\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:37:21 INFO 140569346963264] #quality_metric: host=algo-1, epoch=12, batch=10 train loss <loss>=6.14150800705\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:37:21 INFO 140569346963264] Epoch[12] Batch [10]#011Speed: 53.71 samples/sec#011loss=6.141508\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:37:21 INFO 140569346963264] processed a total of 1282 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28060.80198287964, \"sum\": 28060.80198287964, \"min\": 28060.80198287964}}, \"EndTime\": 1586270241.710082, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586270213.648792}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:37:21 INFO 140569346963264] #throughput_metric: host=algo-1, train throughput=45.6863220713 records/second\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:37:21 INFO 140569346963264] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:37:21 INFO 140569346963264] #quality_metric: host=algo-1, epoch=12, train loss <loss>=6.06136040254\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:37:21 INFO 140569346963264] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:37:25 INFO 140569346963264] Epoch[13] Batch[0] avg_epoch_loss=6.019325\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:37:25 INFO 140569346963264] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=6.01932525635\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:37:37 INFO 140569346963264] Epoch[13] Batch[5] avg_epoch_loss=5.930799\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:37:37 INFO 140569346963264] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=5.93079916636\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:37:37 INFO 140569346963264] Epoch[13] Batch [5]#011Speed: 50.30 samples/sec#011loss=5.930799\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:37:49 INFO 140569346963264] Epoch[13] Batch[10] avg_epoch_loss=5.703490\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:37:49 INFO 140569346963264] #quality_metric: host=algo-1, epoch=13, batch=10 train loss <loss>=5.43071937561\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:37:49 INFO 140569346963264] Epoch[13] Batch [10]#011Speed: 53.70 samples/sec#011loss=5.430719\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:37:49 INFO 140569346963264] processed a total of 1290 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28089.879989624023, \"sum\": 28089.879989624023, \"min\": 28089.879989624023}}, \"EndTime\": 1586270269.800524, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586270241.710164}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:37:49 INFO 140569346963264] #throughput_metric: host=algo-1, train throughput=45.9238148859 records/second\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:37:49 INFO 140569346963264] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:37:49 INFO 140569346963264] #quality_metric: host=algo-1, epoch=13, train loss <loss>=5.70349017057\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:37:49 INFO 140569346963264] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:37:50 INFO 140569346963264] Saved checkpoint to \"/opt/ml/model/state_304c075a-cb01-4b80-97d6-550e5e4eaf0e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 272.3360061645508, \"sum\": 272.3360061645508, \"min\": 272.3360061645508}}, \"EndTime\": 1586270270.073508, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586270269.800611}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:37:53 INFO 140569346963264] Epoch[14] Batch[0] avg_epoch_loss=5.797647\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:37:53 INFO 140569346963264] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=5.79764652252\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:38:06 INFO 140569346963264] Epoch[14] Batch[5] avg_epoch_loss=5.970404\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:38:06 INFO 140569346963264] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=5.9704041481\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:38:06 INFO 140569346963264] Epoch[14] Batch [5]#011Speed: 50.33 samples/sec#011loss=5.970404\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:38:15 INFO 140569346963264] processed a total of 1271 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 25681.097984313965, \"sum\": 25681.097984313965, \"min\": 25681.097984313965}}, \"EndTime\": 1586270295.754757, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586270270.073604}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:38:15 INFO 140569346963264] #throughput_metric: host=algo-1, train throughput=49.4914567326 records/second\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:38:15 INFO 140569346963264] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:38:15 INFO 140569346963264] #quality_metric: host=algo-1, epoch=14, train loss <loss>=5.99246864319\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:38:15 INFO 140569346963264] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:38:19 INFO 140569346963264] Epoch[15] Batch[0] avg_epoch_loss=5.886016\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:38:19 INFO 140569346963264] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=5.88601636887\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:38:31 INFO 140569346963264] Epoch[15] Batch[5] avg_epoch_loss=5.754968\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:38:31 INFO 140569346963264] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=5.75496792793\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:38:31 INFO 140569346963264] Epoch[15] Batch [5]#011Speed: 50.50 samples/sec#011loss=5.754968\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:38:43 INFO 140569346963264] Epoch[15] Batch[10] avg_epoch_loss=5.718144\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:38:43 INFO 140569346963264] #quality_metric: host=algo-1, epoch=15, batch=10 train loss <loss>=5.67395591736\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:38:43 INFO 140569346963264] Epoch[15] Batch [10]#011Speed: 53.61 samples/sec#011loss=5.673956\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:38:43 INFO 140569346963264] processed a total of 1285 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28051.44691467285, \"sum\": 28051.44691467285, \"min\": 28051.44691467285}}, \"EndTime\": 1586270323.806818, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586270295.754827}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:38:43 INFO 140569346963264] #throughput_metric: host=algo-1, train throughput=45.8084924968 records/second\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:38:43 INFO 140569346963264] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:38:43 INFO 140569346963264] #quality_metric: host=algo-1, epoch=15, train loss <loss>=5.71814428676\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:38:43 INFO 140569346963264] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:38:47 INFO 140569346963264] Epoch[16] Batch[0] avg_epoch_loss=5.812557\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:38:47 INFO 140569346963264] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=5.81255674362\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:38:59 INFO 140569346963264] Epoch[16] Batch[5] avg_epoch_loss=5.869518\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:38:59 INFO 140569346963264] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=5.86951812108\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:38:59 INFO 140569346963264] Epoch[16] Batch [5]#011Speed: 53.85 samples/sec#011loss=5.869518\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:39:11 INFO 140569346963264] Epoch[16] Batch[10] avg_epoch_loss=5.867906\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:39:11 INFO 140569346963264] #quality_metric: host=algo-1, epoch=16, batch=10 train loss <loss>=5.86597251892\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:39:11 INFO 140569346963264] Epoch[16] Batch [10]#011Speed: 50.34 samples/sec#011loss=5.865973\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:39:11 INFO 140569346963264] processed a total of 1287 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28071.90990447998, \"sum\": 28071.90990447998, \"min\": 28071.90990447998}}, \"EndTime\": 1586270351.879317, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586270323.806904}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:39:11 INFO 140569346963264] #throughput_metric: host=algo-1, train throughput=45.8463622329 records/second\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:39:11 INFO 140569346963264] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:39:11 INFO 140569346963264] #quality_metric: host=algo-1, epoch=16, train loss <loss>=5.86790648374\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:39:11 INFO 140569346963264] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:39:15 INFO 140569346963264] Epoch[17] Batch[0] avg_epoch_loss=5.598393\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:39:15 INFO 140569346963264] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=5.59839344025\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/07/2020 14:39:28 INFO 140569346963264] Epoch[17] Batch[5] avg_epoch_loss=5.513202\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:39:28 INFO 140569346963264] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=5.5132021904\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:39:28 INFO 140569346963264] Epoch[17] Batch [5]#011Speed: 50.41 samples/sec#011loss=5.513202\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:39:37 INFO 140569346963264] processed a total of 1252 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 25674.915075302124, \"sum\": 25674.915075302124, \"min\": 25674.915075302124}}, \"EndTime\": 1586270377.554776, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586270351.879386}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:39:37 INFO 140569346963264] #throughput_metric: host=algo-1, train throughput=48.76331615 records/second\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:39:37 INFO 140569346963264] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:39:37 INFO 140569346963264] #quality_metric: host=algo-1, epoch=17, train loss <loss>=5.73846325874\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:39:37 INFO 140569346963264] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:39:41 INFO 140569346963264] Epoch[18] Batch[0] avg_epoch_loss=5.258472\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:39:41 INFO 140569346963264] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=5.25847196579\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:39:53 INFO 140569346963264] Epoch[18] Batch[5] avg_epoch_loss=5.570623\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:39:53 INFO 140569346963264] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=5.57062292099\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:39:53 INFO 140569346963264] Epoch[18] Batch [5]#011Speed: 50.40 samples/sec#011loss=5.570623\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:40:05 INFO 140569346963264] Epoch[18] Batch[10] avg_epoch_loss=5.507941\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:40:05 INFO 140569346963264] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=5.43272171021\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:40:05 INFO 140569346963264] Epoch[18] Batch [10]#011Speed: 53.68 samples/sec#011loss=5.432722\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:40:05 INFO 140569346963264] processed a total of 1302 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28082.212924957275, \"sum\": 28082.212924957275, \"min\": 28082.212924957275}}, \"EndTime\": 1586270405.637597, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586270377.554859}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:40:05 INFO 140569346963264] #throughput_metric: host=algo-1, train throughput=46.3636561515 records/second\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:40:05 INFO 140569346963264] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:40:05 INFO 140569346963264] #quality_metric: host=algo-1, epoch=18, train loss <loss>=5.50794055245\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:40:05 INFO 140569346963264] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:40:05 INFO 140569346963264] Saved checkpoint to \"/opt/ml/model/state_ddf78b96-712c-4e6e-940e-79e6b12ebb94-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 271.3940143585205, \"sum\": 271.3940143585205, \"min\": 271.3940143585205}}, \"EndTime\": 1586270405.909644, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586270405.637686}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:40:09 INFO 140569346963264] Epoch[19] Batch[0] avg_epoch_loss=5.108609\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:40:09 INFO 140569346963264] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=5.10860919952\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:40:22 INFO 140569346963264] Epoch[19] Batch[5] avg_epoch_loss=5.460767\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:40:22 INFO 140569346963264] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=5.46076671282\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:40:22 INFO 140569346963264] Epoch[19] Batch [5]#011Speed: 50.52 samples/sec#011loss=5.460767\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:40:33 INFO 140569346963264] Epoch[19] Batch[10] avg_epoch_loss=5.522570\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:40:33 INFO 140569346963264] #quality_metric: host=algo-1, epoch=19, batch=10 train loss <loss>=5.59673290253\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:40:33 INFO 140569346963264] Epoch[19] Batch [10]#011Speed: 53.79 samples/sec#011loss=5.596733\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:40:33 INFO 140569346963264] processed a total of 1318 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28057.919025421143, \"sum\": 28057.919025421143, \"min\": 28057.919025421143}}, \"EndTime\": 1586270433.967723, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586270405.909731}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:40:33 INFO 140569346963264] #throughput_metric: host=algo-1, train throughput=46.974047843 records/second\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:40:33 INFO 140569346963264] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:40:33 INFO 140569346963264] #quality_metric: host=algo-1, epoch=19, train loss <loss>=5.52256952633\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:40:33 INFO 140569346963264] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:40:37 INFO 140569346963264] Epoch[20] Batch[0] avg_epoch_loss=5.622131\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:40:37 INFO 140569346963264] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=5.62213134766\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:40:50 INFO 140569346963264] Epoch[20] Batch[5] avg_epoch_loss=5.723073\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:40:50 INFO 140569346963264] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=5.72307324409\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:40:50 INFO 140569346963264] Epoch[20] Batch [5]#011Speed: 50.46 samples/sec#011loss=5.723073\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:40:59 INFO 140569346963264] processed a total of 1268 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 25637.726068496704, \"sum\": 25637.726068496704, \"min\": 25637.726068496704}}, \"EndTime\": 1586270459.606057, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586270433.967807}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:40:59 INFO 140569346963264] #throughput_metric: host=algo-1, train throughput=49.4581059825 records/second\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:40:59 INFO 140569346963264] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:40:59 INFO 140569346963264] #quality_metric: host=algo-1, epoch=20, train loss <loss>=5.56306395531\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:40:59 INFO 140569346963264] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:41:03 INFO 140569346963264] Epoch[21] Batch[0] avg_epoch_loss=5.723114\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:41:03 INFO 140569346963264] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=5.72311449051\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:41:15 INFO 140569346963264] Epoch[21] Batch[5] avg_epoch_loss=5.637895\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:41:15 INFO 140569346963264] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=5.63789510727\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:41:15 INFO 140569346963264] Epoch[21] Batch [5]#011Speed: 50.52 samples/sec#011loss=5.637895\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:41:25 INFO 140569346963264] processed a total of 1263 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 25600.000143051147, \"sum\": 25600.000143051147, \"min\": 25600.000143051147}}, \"EndTime\": 1586270485.206705, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586270459.606149}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:41:25 INFO 140569346963264] #throughput_metric: host=algo-1, train throughput=49.3357120817 records/second\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:41:25 INFO 140569346963264] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:41:25 INFO 140569346963264] #quality_metric: host=algo-1, epoch=21, train loss <loss>=5.55478634834\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:41:25 INFO 140569346963264] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:41:28 INFO 140569346963264] Epoch[22] Batch[0] avg_epoch_loss=5.401876\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:41:28 INFO 140569346963264] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=5.40187597275\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:41:41 INFO 140569346963264] Epoch[22] Batch[5] avg_epoch_loss=5.539361\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:41:41 INFO 140569346963264] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=5.53936052322\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:41:41 INFO 140569346963264] Epoch[22] Batch [5]#011Speed: 50.49 samples/sec#011loss=5.539361\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/07/2020 14:41:50 INFO 140569346963264] processed a total of 1262 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 25632.642030715942, \"sum\": 25632.642030715942, \"min\": 25632.642030715942}}, \"EndTime\": 1586270510.83997, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586270485.206783}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:41:50 INFO 140569346963264] #throughput_metric: host=algo-1, train throughput=49.2338517052 records/second\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:41:50 INFO 140569346963264] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:41:50 INFO 140569346963264] #quality_metric: host=algo-1, epoch=22, train loss <loss>=5.61791858673\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:41:50 INFO 140569346963264] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:41:54 INFO 140569346963264] Epoch[23] Batch[0] avg_epoch_loss=5.535528\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:41:54 INFO 140569346963264] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=5.53552818298\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:42:07 INFO 140569346963264] Epoch[23] Batch[5] avg_epoch_loss=5.659837\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:42:07 INFO 140569346963264] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=5.659837087\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:42:07 INFO 140569346963264] Epoch[23] Batch [5]#011Speed: 50.33 samples/sec#011loss=5.659837\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:42:18 INFO 140569346963264] Epoch[23] Batch[10] avg_epoch_loss=5.835324\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:42:18 INFO 140569346963264] #quality_metric: host=algo-1, epoch=23, batch=10 train loss <loss>=6.04590873718\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:42:18 INFO 140569346963264] Epoch[23] Batch [10]#011Speed: 53.56 samples/sec#011loss=6.045909\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:42:18 INFO 140569346963264] processed a total of 1312 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28134.780883789062, \"sum\": 28134.780883789062, \"min\": 28134.780883789062}}, \"EndTime\": 1586270538.97539, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586270510.840058}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:42:18 INFO 140569346963264] #throughput_metric: host=algo-1, train throughput=46.632444466 records/second\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:42:18 INFO 140569346963264] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:42:18 INFO 140569346963264] #quality_metric: host=algo-1, epoch=23, train loss <loss>=5.83532420072\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:42:18 INFO 140569346963264] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:42:22 INFO 140569346963264] Epoch[24] Batch[0] avg_epoch_loss=5.151172\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:42:22 INFO 140569346963264] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=5.1511721611\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:42:35 INFO 140569346963264] Epoch[24] Batch[5] avg_epoch_loss=5.594253\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:42:35 INFO 140569346963264] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=5.59425258636\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:42:35 INFO 140569346963264] Epoch[24] Batch [5]#011Speed: 50.42 samples/sec#011loss=5.594253\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:42:44 INFO 140569346963264] processed a total of 1248 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 25661.4830493927, \"sum\": 25661.4830493927, \"min\": 25661.4830493927}}, \"EndTime\": 1586270564.63751, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586270538.975484}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:42:44 INFO 140569346963264] #throughput_metric: host=algo-1, train throughput=48.6329161385 records/second\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:42:44 INFO 140569346963264] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:42:44 INFO 140569346963264] #quality_metric: host=algo-1, epoch=24, train loss <loss>=5.60337486267\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:42:44 INFO 140569346963264] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:42:48 INFO 140569346963264] Epoch[25] Batch[0] avg_epoch_loss=5.426925\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:42:48 INFO 140569346963264] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=5.42692470551\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:43:00 INFO 140569346963264] Epoch[25] Batch[5] avg_epoch_loss=5.612445\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:43:00 INFO 140569346963264] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=5.61244535446\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:43:00 INFO 140569346963264] Epoch[25] Batch [5]#011Speed: 53.77 samples/sec#011loss=5.612445\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:43:12 INFO 140569346963264] Epoch[25] Batch[10] avg_epoch_loss=5.796163\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:43:12 INFO 140569346963264] #quality_metric: host=algo-1, epoch=25, batch=10 train loss <loss>=6.01662368774\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:43:12 INFO 140569346963264] Epoch[25] Batch [10]#011Speed: 50.42 samples/sec#011loss=6.016624\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:43:12 INFO 140569346963264] processed a total of 1369 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28068.993091583252, \"sum\": 28068.993091583252, \"min\": 28068.993091583252}}, \"EndTime\": 1586270592.707227, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586270564.637621}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:43:12 INFO 140569346963264] #throughput_metric: host=algo-1, train throughput=48.7724660082 records/second\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:43:12 INFO 140569346963264] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:43:12 INFO 140569346963264] #quality_metric: host=algo-1, epoch=25, train loss <loss>=5.79616277868\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:43:12 INFO 140569346963264] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:43:16 INFO 140569346963264] Epoch[26] Batch[0] avg_epoch_loss=6.264722\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:43:16 INFO 140569346963264] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=6.26472187042\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:43:28 INFO 140569346963264] Epoch[26] Batch[5] avg_epoch_loss=5.698583\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:43:28 INFO 140569346963264] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=5.69858344396\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:43:28 INFO 140569346963264] Epoch[26] Batch [5]#011Speed: 50.43 samples/sec#011loss=5.698583\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:43:40 INFO 140569346963264] Epoch[26] Batch[10] avg_epoch_loss=5.569950\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:43:40 INFO 140569346963264] #quality_metric: host=algo-1, epoch=26, batch=10 train loss <loss>=5.41559028625\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:43:40 INFO 140569346963264] Epoch[26] Batch [10]#011Speed: 53.70 samples/sec#011loss=5.415590\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:43:40 INFO 140569346963264] processed a total of 1293 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28052.255153656006, \"sum\": 28052.255153656006, \"min\": 28052.255153656006}}, \"EndTime\": 1586270620.760106, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586270592.70731}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:43:40 INFO 140569346963264] #throughput_metric: host=algo-1, train throughput=46.0923542974 records/second\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:43:40 INFO 140569346963264] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:43:40 INFO 140569346963264] #quality_metric: host=algo-1, epoch=26, train loss <loss>=5.56995019046\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:43:40 INFO 140569346963264] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:43:44 INFO 140569346963264] Epoch[27] Batch[0] avg_epoch_loss=5.543167\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:43:44 INFO 140569346963264] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=5.54316711426\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:43:56 INFO 140569346963264] Epoch[27] Batch[5] avg_epoch_loss=5.651038\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:43:56 INFO 140569346963264] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=5.65103769302\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:43:56 INFO 140569346963264] Epoch[27] Batch [5]#011Speed: 50.38 samples/sec#011loss=5.651038\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:44:08 INFO 140569346963264] Epoch[27] Batch[10] avg_epoch_loss=5.756036\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:44:08 INFO 140569346963264] #quality_metric: host=algo-1, epoch=27, batch=10 train loss <loss>=5.88203296661\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:44:08 INFO 140569346963264] Epoch[27] Batch [10]#011Speed: 53.76 samples/sec#011loss=5.882033\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:44:08 INFO 140569346963264] processed a total of 1337 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28050.86898803711, \"sum\": 28050.86898803711, \"min\": 28050.86898803711}}, \"EndTime\": 1586270648.811546, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586270620.760187}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:44:08 INFO 140569346963264] #throughput_metric: host=algo-1, train throughput=47.6631899787 records/second\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:44:08 INFO 140569346963264] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:44:08 INFO 140569346963264] #quality_metric: host=algo-1, epoch=27, train loss <loss>=5.75603554466\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:44:08 INFO 140569346963264] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:44:12 INFO 140569346963264] Epoch[28] Batch[0] avg_epoch_loss=5.277355\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:44:12 INFO 140569346963264] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=5.27735519409\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/07/2020 14:44:24 INFO 140569346963264] Epoch[28] Batch[5] avg_epoch_loss=5.431944\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:44:24 INFO 140569346963264] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=5.43194381396\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:44:24 INFO 140569346963264] Epoch[28] Batch [5]#011Speed: 50.46 samples/sec#011loss=5.431944\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:44:34 INFO 140569346963264] processed a total of 1263 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 25617.027044296265, \"sum\": 25617.027044296265, \"min\": 25617.027044296265}}, \"EndTime\": 1586270674.429149, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586270648.811634}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:44:34 INFO 140569346963264] #throughput_metric: host=algo-1, train throughput=49.3029335091 records/second\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:44:34 INFO 140569346963264] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:44:34 INFO 140569346963264] #quality_metric: host=algo-1, epoch=28, train loss <loss>=5.56955313683\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:44:34 INFO 140569346963264] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:44:34 INFO 140569346963264] Loading parameters from best epoch (18)\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.deserialize.time\": {\"count\": 1, \"max\": 113.80219459533691, \"sum\": 113.80219459533691, \"min\": 113.80219459533691}}, \"EndTime\": 1586270674.543669, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586270674.429217}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:44:34 INFO 140569346963264] stopping training now\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:44:34 INFO 140569346963264] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:44:34 INFO 140569346963264] Final loss: 5.50794055245 (occurred at epoch 18)\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:44:34 INFO 140569346963264] #quality_metric: host=algo-1, train final_loss <loss>=5.50794055245\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:44:34 WARNING 140569346963264] You are using large values for `context_length` and/or `prediction_length`. The following step may take some time. If the step crashes, use an instance with more memory or reduce these two parameters.\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:44:34 INFO 140569346963264] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:44:34 WARNING 140569346963264] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:44:34 INFO 140569346963264] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 10548.532962799072, \"sum\": 10548.532962799072, \"min\": 10548.532962799072}}, \"EndTime\": 1586270685.09322, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586270674.543754}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:44:46 INFO 140569346963264] Number of GPUs being used: 1\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 11711.464881896973, \"sum\": 11711.464881896973, \"min\": 11711.464881896973}}, \"EndTime\": 1586270686.256108, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586270685.093322}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:44:46 INFO 140569346963264] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:44:46 INFO 140569346963264] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 174.46303367614746, \"sum\": 174.46303367614746, \"min\": 174.46303367614746}}, \"EndTime\": 1586270686.430698, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586270686.256183}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:44:46 INFO 140569346963264] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:44:46 INFO 140569346963264] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.bind.time\": {\"count\": 1, \"max\": 0.051021575927734375, \"sum\": 0.051021575927734375, \"min\": 0.051021575927734375}}, \"EndTime\": 1586270686.431599, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586270686.430764}\n",
      "\u001b[0m\n",
      "\n",
      "2020-04-07 14:45:08 Uploading - Uploading generated training model\u001b[34m#metrics {\"Metrics\": {\"model.score.time\": {\"count\": 1, \"max\": 16964.725971221924, \"sum\": 16964.725971221924, \"min\": 16964.725971221924}}, \"EndTime\": 1586270703.396269, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586270686.431663}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:45:03 INFO 140569346963264] #test_score (algo-1, RMSE): 1431.49971198\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:45:03 INFO 140569346963264] #test_score (algo-1, mean_absolute_QuantileLoss): 43748922.0426505\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:45:03 INFO 140569346963264] #test_score (algo-1, mean_wQuantileLoss): 0.13626641081293636\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:45:03 INFO 140569346963264] #test_score (algo-1, wQuantileLoss[0.1]): 0.08285624500755029\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:45:03 INFO 140569346963264] #test_score (algo-1, wQuantileLoss[0.2]): 0.12273398665660723\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:45:03 INFO 140569346963264] #test_score (algo-1, wQuantileLoss[0.3]): 0.14832914733862412\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:45:03 INFO 140569346963264] #test_score (algo-1, wQuantileLoss[0.4]): 0.1631757639423883\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:45:03 INFO 140569346963264] #test_score (algo-1, wQuantileLoss[0.5]): 0.16853760343152763\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:45:03 INFO 140569346963264] #test_score (algo-1, wQuantileLoss[0.6]): 0.1649138848820917\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:45:03 INFO 140569346963264] #test_score (algo-1, wQuantileLoss[0.7]): 0.15273130960468112\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:45:03 INFO 140569346963264] #test_score (algo-1, wQuantileLoss[0.8]): 0.13078824677093226\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:45:03 INFO 140569346963264] #test_score (algo-1, wQuantileLoss[0.9]): 0.09233150968202455\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:45:03 INFO 140569346963264] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.136266410813\u001b[0m\n",
      "\u001b[34m[04/07/2020 14:45:03 INFO 140569346963264] #quality_metric: host=algo-1, test RMSE <loss>=1431.49971198\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 835875.4160404205, \"sum\": 835875.4160404205, \"min\": 835875.4160404205}, \"setuptime\": {\"count\": 1, \"max\": 9.82809066772461, \"sum\": 9.82809066772461, \"min\": 9.82809066772461}}, \"EndTime\": 1586270703.665398, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586270703.396345}\n",
      "\u001b[0m\n",
      "\n",
      "2020-04-07 14:45:14 Completed - Training job completed\n",
      "Training seconds: 924\n",
      "Billable seconds: 924\n",
      "CPU times: user 2.2 s, sys: 111 ms, total: 2.31 s\n",
      "Wall time: 18min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train and test channels\n",
    "data_channels = {\n",
    "    \"train\": train_path,\n",
    "    \"test\": test_path\n",
    "}\n",
    "\n",
    "# fit the estimator\n",
    "estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. create predictor (by deploying estimator) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using already existing model: forecasting-deepar-2020-04-07-14-27-32-842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!CPU times: user 196 ms, sys: 22.5 ms, total: 219 ms\n",
      "Wall time: 6min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create a predictor\n",
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.p2.xlarge',\n",
    "    content_type=\"application/json\" # specify that it will accept/produce JSON\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. generating predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instance_creator(ts):\n",
    "    '''Accepts a list of input time series and returns an instance (jaspn transformed) input data for prediction'''\n",
    "    # request data is made of JSON objects (instances) \n",
    "    # here I delete the last 49 values of the target list, as this is what should be predicted by the model\n",
    "    # the dynamic features include these 49 values for the prediction length as indicated by the aws documentation \n",
    "    \n",
    "    instances = []\n",
    "    for store in store_nr:\n",
    "        df_store = ts.loc[ts['Store'] == store]\n",
    "        l = list(df_store.Sales)\n",
    "        del l[-49:]\n",
    "        json_obj = {\"start\": str(df_store.index[0]), \"target\": l , \"cat\": [int(store)], \"dynamic_feat\": [list(df_store.DayOfWeek),list(df_store.Open),list(df_store.Promo),list(df_store.StateHoliday),list(df_store.SchoolHoliday),list(df_store.StoreType),list(df_store.Assortment),list(df_store.CompetitionDistance),list(df_store.Promo_2_active),list(df_store.Open_sunday)]}\n",
    "        instances.append(json_obj)\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_predictor_input(instances, num_samples=49, quantiles=['0.1', '0.5', '0.9']):\n",
    "    '''Accepts a list of input time series and produces a formatted input.\n",
    "       :input_ts: An list of input time series.\n",
    "       :num_samples: Number of samples to calculate metrics with.\n",
    "       :quantiles: A list of quantiles to return in the predicted output.\n",
    "       :return: The JSON-formatted input.\n",
    "       '''\n",
    "    # request data is made of JSON objects (instances)\n",
    "    # and an output configuration that details the type of data/quantiles we want\n",
    "    \n",
    " \n",
    "    # specify the output quantiles and samples\n",
    "    configuration = {\"num_samples\": num_samples, \n",
    "                     \"output_types\": [\"mean\",\"quantiles\"], \n",
    "                     \"quantiles\": quantiles}\n",
    "\n",
    "    request_data = {\"instances\": instances, \n",
    "                    \"configuration\": configuration}\n",
    "\n",
    "    json_request = json.dumps(request_data).encode('utf-8')\n",
    "    \n",
    "    return json_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all input and target (test) time series\n",
    "input_ts = df_test\n",
    "\n",
    "# get formatted input time series\n",
    "ts_instances = instance_creator(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_input_ts = json_predictor_input(ts_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionClosedError",
     "evalue": "Connection was closed before we received a valid response from endpoint URL: \"https://runtime.sagemaker.eu-central-1.amazonaws.com/endpoints/forecasting-deepar-2020-04-07-14-27-32-842/invocations\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/botocore/awsrequest.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         rval = super(AWSConnection, self)._send_request(\n\u001b[0;32m---> 92\u001b[0;31m             method, url, body, headers, *args, **kwargs)\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expect_header_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1284\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1233\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1234\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/botocore/awsrequest.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, *args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mmessage_body\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expect_header_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/botocore/awsrequest.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, str)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAWSConnection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36msendall\u001b[0;34m(self, data, flags)\u001b[0m\n\u001b[1;32m    971\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mamount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m                     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m                     \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data, flags)\u001b[0m\n\u001b[1;32m    940\u001b[0m                     self.__class__)\n\u001b[0;32m--> 941\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    942\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \"\"\"\n\u001b[0;32m--> 642\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionResetError\u001b[0m: [Errno 104] Connection reset by peer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/botocore/httpsession.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    262\u001b[0m                 \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m                 \u001b[0mchunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m             )\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    637\u001b[0m             retries = retries.increment(method, url, error=e, _pool=self,\n\u001b[0;32m--> 638\u001b[0;31m                                         _stacktrace=sys.exc_info()[2])\n\u001b[0m\u001b[1;32m    639\u001b[0m             \u001b[0mretries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Disabled, indicate to re-raise the error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/botocore/awsrequest.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         rval = super(AWSConnection, self)._send_request(\n\u001b[0;32m---> 92\u001b[0;31m             method, url, body, headers, *args, **kwargs)\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expect_header_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1284\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1233\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1234\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/botocore/awsrequest.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, *args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mmessage_body\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expect_header_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/botocore/awsrequest.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, str)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAWSConnection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36msendall\u001b[0;34m(self, data, flags)\u001b[0m\n\u001b[1;32m    971\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mamount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m                     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m                     \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data, flags)\u001b[0m\n\u001b[1;32m    940\u001b[0m                     self.__class__)\n\u001b[0;32m--> 941\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    942\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \"\"\"\n\u001b[0;32m--> 642\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mProtocolError\u001b[0m: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionClosedError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-252-a241217de43e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# get the prediction from the predictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mjson_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_input_ts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, initial_args, target_model)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mrequest_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_request_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_runtime_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    315\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m             http, parsed_response = self._make_request(\n\u001b[0;32m--> 613\u001b[0;31m                 operation_model, request_dict, request_context)\n\u001b[0m\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m         self.meta.events.emit(\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, operation_model, request_dict, request_context)\u001b[0m\n\u001b[1;32m    630\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_endpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             self.meta.events.emit(\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/botocore/endpoint.py\u001b[0m in \u001b[0;36mmake_request\u001b[0;34m(self, operation_model, request_dict)\u001b[0m\n\u001b[1;32m    100\u001b[0m         logger.debug(\"Making request for %s with params: %s\",\n\u001b[1;32m    101\u001b[0m                      operation_model, request_dict)\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/botocore/endpoint.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, request_dict, operation_model)\u001b[0m\n\u001b[1;32m    135\u001b[0m             request, operation_model, context)\n\u001b[1;32m    136\u001b[0m         while self._needs_retry(attempts, operation_model, request_dict,\n\u001b[0;32m--> 137\u001b[0;31m                                 success_response, exception):\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0mattempts\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;31m# If there is a stream associated with the request, we need\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/botocore/endpoint.py\u001b[0m in \u001b[0;36m_needs_retry\u001b[0;34m(self, attempts, operation_model, request_dict, response, caught_exception)\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0moperation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moperation_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattempts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattempts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m             caught_exception=caught_exception, request_dict=request_dict)\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0mhandler_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_non_none_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandler_response\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/botocore/hooks.py\u001b[0m in \u001b[0;36memit\u001b[0;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0memit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0maliased_event_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alias_event_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_emitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maliased_event_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0memit_until_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/botocore/hooks.py\u001b[0m in \u001b[0;36memit\u001b[0;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m                  \u001b[0mhandlers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \"\"\"\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_emit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0memit_until_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/botocore/hooks.py\u001b[0m in \u001b[0;36m_emit\u001b[0;34m(self, event_name, kwargs, stop_on_response)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers_to_call\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Event %s: calling handler %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m             \u001b[0mresponses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstop_on_response\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/botocore/retryhandler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, attempts, response, caught_exception, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \"\"\"\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattempts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaught_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattempts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattempts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Retry needed, action of: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/botocore/retryhandler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, attempt_number, response, caught_exception)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattempt_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaught_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         should_retry = self._should_retry(attempt_number, response,\n\u001b[0;32m--> 251\u001b[0;31m                                           caught_exception)\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshould_retry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mattempt_number\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_attempts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/botocore/retryhandler.py\u001b[0m in \u001b[0;36m_should_retry\u001b[0;34m(self, attempt_number, response, caught_exception)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;31m# If we've exceeded the max attempts we just let the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;31m# propogate if one has occurred.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattempt_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaught_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/botocore/retryhandler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, attempt_number, response, caught_exception)\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mchecker\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             checker_response = checker(attempt_number, response,\n\u001b[0;32m--> 317\u001b[0;31m                                        caught_exception)\n\u001b[0m\u001b[1;32m    318\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mchecker_response\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mchecker_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/botocore/retryhandler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, attempt_number, response, caught_exception)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcaught_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             return self._check_caught_exception(\n\u001b[0;32m--> 223\u001b[0;31m                 attempt_number, caught_exception)\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Both response and caught_exception are None.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/botocore/retryhandler.py\u001b[0m in \u001b[0;36m_check_caught_exception\u001b[0;34m(self, attempt_number, caught_exception)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;31m# the MaxAttemptsDecorator is not interested in retrying the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;31m# then this exception just propogates out past the retry code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mcaught_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/botocore/endpoint.py\u001b[0m in \u001b[0;36m_do_get_response\u001b[0;34m(self, request, operation_model)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mhttp_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_non_none_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhttp_response\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                 \u001b[0mhttp_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPClientError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/botocore/endpoint.py\u001b[0m in \u001b[0;36m_send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/botocore/httpsession.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m                 \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m                 \u001b[0mendpoint_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             )\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionClosedError\u001b[0m: Connection was closed before we received a valid response from endpoint URL: \"https://runtime.sagemaker.eu-central-1.amazonaws.com/endpoints/forecasting-deepar-2020-04-07-14-27-32-842/invocations\"."
     ]
    }
   ],
   "source": [
    "# get the prediction from the predictor\n",
    "json_prediction = predictor.predict(json_input_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
